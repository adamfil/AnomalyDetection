{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f71ac7",
   "metadata": {},
   "source": [
    "# Live Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f2526",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64cfb5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "469522de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../src/data')\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix \n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.inspection import permutation_importance, plot_partial_dependence\n",
    "\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "from xml2dict import *\n",
    "from dict2tabular import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a65433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml2soup(xml_path: str):\n",
    "    \"\"\" Loads xml into BeautifulSoup object.\n",
    "\n",
    "    Params:\n",
    "        xml_path (string) - Path to xml to be loaded\n",
    "\n",
    "    Returns:\n",
    "        soup (BeautifulSoup object)\n",
    "    \"\"\"\n",
    "    xml_content = requests.get(xml_path)\n",
    "    soup = BeautifulSoup(xml_content.content, 'lxml')\n",
    "    return soup\n",
    "\n",
    "def xml_extract_metadata(xml_soup):\n",
    "    \"\"\" Extracts metadata for identification (station, time and location) from top of xml.\n",
    "\n",
    "    Params:\n",
    "        output_dict (dict) - Dictionary to add information to\n",
    "        xml_soup (BeautifulSoup object) - beautifulSoup object containing the loaded xml information.\n",
    "\n",
    "    Returns:\n",
    "        output_dict (dict) - Updated dictionary with added metadata information\n",
    "    \"\"\"\n",
    "    output_dict = dict()\n",
    "    identification_elements = xml_soup.find(\"identification-elements\")\n",
    "    id_element_list = ['date_time', 'tc_identifier', 'station_name', 'station_elevation',\n",
    "                       'latitude', 'longitude', 'version', 'correction', 'source_uri']\n",
    "    for id_element in id_element_list:\n",
    "        try:\n",
    "            output_dict[id_element] = identification_elements.findChild(name='element',\n",
    "                                                                        attrs={'name': id_element}\n",
    "                                                                        ).get(\"value\")\n",
    "        except AttributeError as error:\n",
    "            print(error, \". Possibly element '%s' is missing.\" % id_element)\n",
    "\n",
    "    # Station identifier are missing in some xml files\n",
    "    try:\n",
    "        output_dict['station_identifier'] = identification_elements.findChild(name='element',\n",
    "                                                                              attrs={'name': 'station_identifier'}\n",
    "                                                                              ).get(\"value\")\n",
    "    except AttributeError as error:\n",
    "        print('error sigma')\n",
    "\n",
    "    observation_elements = xml_soup.find('om:result').find('elements').findAll('element')\n",
    "    for each_elem in observation_elements:\n",
    "        \n",
    "        try:\n",
    "            if each_elem['element-index']:\n",
    "                \n",
    "                dict_key = each_elem['name']+'_'+each_elem['orig-name']\n",
    "                output_dict[dict_key+'_value'] = each_elem['value']\n",
    "                #print(each_elem['orig-name'])\n",
    "                #print(each_elem['value'])\n",
    "                \n",
    "                qc_soup = each_elem.find('quality-controlled')\n",
    "                qc_summary_dict_key = dict_key+'_'+qc_soup.find('element')['name']\n",
    "                output_dict[qc_summary_dict_key] = qc_soup.find('element')['value']\n",
    "                \n",
    "                # qc native tag\n",
    "                qc_native = qc_soup.find('native').findAll('qualifier')\n",
    "                for each_native in qc_native:\n",
    "                    try:\n",
    "                        output_dict[dict_key+'_'+each_native['name']] = each_native['value']\n",
    "                    except:\n",
    "                        print('error beta')\n",
    "                        continue\n",
    "                \n",
    "                #print(output_dict)\n",
    "                #print(qc_soup.find('real-time'))\n",
    "                qc_element_list = qc_soup.find('real-time').find('element').findAll('element', recursive=False)\n",
    "                #print(qc_element_list)\n",
    "                for each_qc in qc_element_list:\n",
    "                    try:\n",
    "                        qc_dict_key = dict_key+'_qa-'+each_qc['name']\n",
    "                        output_dict[qc_dict_key] = each_qc['value']\n",
    "                    \n",
    "                        qc_detail = each_qc.findAll('element')\n",
    "\n",
    "                        for qc_det_item in qc_detail:\n",
    "                            try:\n",
    "                                qc_det_name = dict_key+'_qc-'+qc_det_item['name']+'_'+qc_det_item['value'].split('/')[6]\n",
    "                                output_dict[qc_det_name] = qc_det_item.find('qualifier', {'name' : 'flag_value'})['value']\n",
    "                            except:\n",
    "                                print(\"error gamma\")\n",
    "                                continue\n",
    "                    except:\n",
    "                        print('error gamma')\n",
    "                        continue\n",
    "        except:\n",
    "\n",
    "            continue\n",
    "       # print(output_dict)\n",
    "\n",
    "    #finally:\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e3c15d",
   "metadata": {},
   "source": [
    "## Import Live Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d68bb7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://dms.cmc.ec.gc.ca:8180/notification?path=/msc/observation/atmospheric/surface_weather/ca-1.1-ascii/decoded_qa_enhanced-xml-2.0&time=1d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items = []\n",
    "for i in range(1, 6):\n",
    "\n",
    "    content_url = 'http://dms.cmc.ec.gc.ca:8180/notification?path=/msc/observation/atmospheric/surface_weather/ca-1.1-ascii/decoded_qa_enhanced-xml-2.0&time='+str(i)+'d'\n",
    "    print(content_url)\n",
    "    content_data = requests.get(content_url)\n",
    "    html = xml2soup(content_url)\n",
    "    item = html.findAll('item')\n",
    "    print(item)\n",
    "    items.extend(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b446561f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ORDER_NO  STN_ID  WMO_ID CLIMATE_ID      ENG_STN_NAME PROVINCE TC_ID  \\\n",
      "0         122   26986   71984    2203058          PAULATUK     NWT   ZPK    \n",
      "1          28    6845   71222   119BLM0   DEASE LAKE (AUT)      BC   WKX    \n",
      "2         105   44204   71823   7093GJ5       LA GRANDE IV     QUE   GAH    \n",
      "3          16   45567   71110    8403619       ST LAWRENCE     NFLD  ADS    \n",
      "4          71   27205   71558    8403399        ST ANTHONY     NFLD  WDW    \n",
      "..        ...     ...     ...        ...               ...      ...   ...   \n",
      "123        26   50857   71208    6010740    BIG TROUT LAKE     ONT   OTL    \n",
      "124        98   10220   71747   6020LPQ     ATIKOKAN (AUT)     ONT   WCH    \n",
      "125       106   10721   71826    2403049       PANGNIRTUNG      NU   WXP    \n",
      "126        70   26866   71550    5040681        DAUPHIN CS     MAN   WZT    \n",
      "127        81   54098   71594    1192948       FORT NELSON      BC   VFN    \n",
      "\n",
      "    STATION_TYPE  Count Network                Error Types  \n",
      "0            AU8   4506      CA                       Wind  \n",
      "1            AU8   3158      CA                       Wind  \n",
      "2            AU8   2662      CA  Wind, Temp,  Rain (mixed)  \n",
      "3            AU8   2569      CA                         RH  \n",
      "4            AU8   2438      CA                       PCPN  \n",
      "..           ...    ...     ...                        ...  \n",
      "123          AU8      0     NaN                        NaN  \n",
      "124          AU8      0     NaN                        NaN  \n",
      "125          AU8      0     NaN                        NaN  \n",
      "126          AU8      0     NaN                        NaN  \n",
      "127          AU8      0     NaN                        NaN  \n",
      "\n",
      "[128 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "path = rfile='../anomalydetection/data/processed1/df_snow_dC:\\Users\\filipovicha\\Documents\\AI_Project\\CLIMAT_Stations_2019-12_MIDAS_Errors.csv'\n",
    "df = pd.read_csv(path, encoding='latin1')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dde025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_id = ['nco', 'nek', 'zrp', 'mfj', 'zpk', 'nbi', 'eqi', 'ngh', 'nsg', 'wfz', 'wyh', 'wkx', 'gah', 'ads', 'wdw', 'wzv', 'wic', 'ndt', 'wsy', 'won', 'way', 'xcm', 'xux', 'xet', 'zlt', 'wij', 'wjc', 'nzs', 'wnv', 'wst', 'zcr', 'zel', 'xbl', 'mfm', 'acq', 'pjm', 'pif', 'who', 'wpz', 'xgd', 'zhk', 'pqw', 'wdv', 'wfp', 'wnz', 'xdi', 'wvt', 'pyq', 'apr', 'xqh', 'xfb', 'wpk', 'nvq', 'wct', 'wzg', 'xmm', 'zcy', 'web', 'wgd', 'xeg', 'vqz', 'xse', 'vxy', 'pqd', 'zvm', 'pzh', 'mjk', 'xox', 'zsm', 'aqy', 'ncd', 'zdb','xrb', 'xqb', 'zhy', 'abf', 'wgr',  'xnp', 'wdc', 'xwf', 'wsk', 'ahr', 'xar', 'zfs', 'wvc', 'xoa', 'ple', 'zsp', 'wtd', 'wrk', 'wsn',  'zhb', 'xmw', 'mrf', 'xto', 'ppr', 'nbb', 'xat', 'erm', 'xhi', 'zka', 'nco', 'adl', 'zoc', 'wyj', 'asb', 'apb', 'xzv', 'xha', 'xzc', 'wwn', 'wcf', 'xka', 'xrg', 'tze', 'wfz', 'zev', 'pgf', 'xtl', 'ybg', 'yyr', 'yod', 'yoj', 'ygq', 'yzt', 'ygl', 'yth', 'yvp', 'yxy', 'otl', 'wch', 'wxp', 'wzt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "621382be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ZPK', 'WKX', 'GAH', 'ADS', 'WDW', 'WZV', 'WIC', 'NDT', 'WSY', 'WON', 'WAY', 'XCM', 'XUX', 'XET', 'ZLT', 'WIJ', 'WJC', 'NZS', 'WNV', 'WST', 'ZCR', 'ZEL', 'XBL', 'MFM', 'ACQ', 'PJM', 'PIF', 'WHO', 'WPZ', 'XGD', 'ZHK', 'PQW', 'WDV', 'WFP', 'WNZ', 'XDI', 'WVT', 'PYQ', 'APR', 'XQH', 'XFB', 'WPK', 'NVQ', 'WCT', 'WZG', 'XMM', 'ZCY', 'WEB', 'WGD', 'XEG', 'VQZ', 'XSE', 'VXY', 'PQD', 'ZVM', 'PZH', 'MJK', 'XOX', 'ZSM', 'AQY', 'NCD', 'ZDB', 'WMJ', 'XRB', 'XWB', 'ZHY', 'ABF', 'WGR', 'XNP', 'WDC', 'XWF', 'WSK', 'AHR', 'XAR', 'ZFS', 'WVC', 'XOA', 'ZRP', 'PLE', 'NEK', 'ZSP', 'WTD', 'WRK', 'WSN', 'ZHB', 'EQI', 'XMW', 'MRF', 'XTO', 'PPR', 'NBB', 'XAT', 'ERM', 'XHI', 'ZKA', 'NCO', 'ADL', 'ZOC', 'WYJ', 'ASB', 'APB', 'XZV', 'XHA', 'XZC', 'WWN', 'WCF', 'XKA', 'XRG', 'TZE', 'WFZ', 'ZEV', 'PGF', 'XTL', 'YBG', 'YYR', 'YOD', 'YOJ', 'YGQ', 'YZT', 'YGL', 'YTH', 'YVP', 'YXY', 'OTL', 'WCH', 'WXP', 'WZT', 'VFN', 'MFJ', 'NBI', 'NGH', 'NSG', 'WYH', 'XQB']\n",
      "134\n"
     ]
    }
   ],
   "source": [
    "tc_id_array = []\n",
    "for index, value in df['TC_ID'].items():\n",
    "    value = value[0:3]\n",
    "    tc_id_array.append(value)\n",
    "    \n",
    "for value in station_id:\n",
    "    if value.upper() not in tc_id_array:\n",
    "        tc_id_array.append(value.upper())\n",
    "\n",
    "station_id = []\n",
    "for id in tc_id_array:\n",
    "    station_id.append(id.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7753a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "url_list = list()\n",
    "\n",
    "for each in items:\n",
    "\n",
    "    # get url of each observation and ignore supporting xmls (supp_1440)\n",
    "    if each.find('title').contents[0].split('/')[2] in station_id and 'supp_1440' not in each.find('title').contents[0]:\n",
    "        url_list.append(each.contents[2])\n",
    "        #parse url and create a table with each row as observation, and element value as columns\n",
    "        soupu = xml2soup(each.contents[2])\n",
    "        extracted = xml_extract_metadata(soupu)\n",
    "        df = df.append(extracted, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f341fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date_time', 'tc_identifier', 'station_name', 'station_elevation',\n",
       "       'latitude', 'longitude', 'version', 'correction', 'source_uri',\n",
       "       'station_identifier',\n",
       "       ...\n",
       "       'cumulative_precipitation_gauge_weight_unfiltered_3020_value',\n",
       "       'cumulative_precipitation_gauge_weight_unfiltered_3020_overall_qa_summary',\n",
       "       'cumulative_precipitation_gauge_weight_unfiltered_3020_error',\n",
       "       'cumulative_precipitation_gauge_weight_unfiltered_3020_suspect',\n",
       "       'cumulative_precipitation_gauge_weight_unfiltered_3020_suppressed',\n",
       "       'cumulative_precipitation_gauge_weight_unfiltered_3021_value',\n",
       "       'cumulative_precipitation_gauge_weight_unfiltered_3021_overall_qa_summary',\n",
       "       'cumulative_precipitation_gauge_weight_unfiltered_3021_error',\n",
       "       'cumulative_precipitation_gauge_weight_unfiltered_3021_suspect',\n",
       "       'cumulative_precipitation_gauge_weight_unfiltered_3021_suppressed'],\n",
       "      dtype='object', length=261)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop if any columns contains null\n",
    "newDf = df.dropna(how='any', axis=1)\n",
    "newDf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d016144c",
   "metadata": {},
   "source": [
    "## Import Snow Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95e8f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = read_pickle(file='../anomalydetection/data/processed1/df_snow_depth_3022_2019.pickle')\n",
    "X_2019 = read_pickle(file='../anomalydetection/data/processed1/df_snow_depth_3022_2020.pickle')\n",
    "X_2020 = read_pickle(file='../anomalydetection/data/processed1/df_snow_depth_3022_2021_jan_jun.pickle')\n",
    "\n",
    "Xsnow = pd.concat([X, X_2019], ignore_index=True, sort=False)\n",
    "Xsnow = pd.concat([X, X_2020], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a13e2652",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xsnow = Xsnow.dropna(how='any', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8209fa8",
   "metadata": {},
   "source": [
    "## Import Wind Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7680a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xwind = read_pickle(r'C:\\Users\\filipovicha\\Documents\\AI_Project\\moov-ai-automatic-qc\\data\\processed_before_nf\\wind_speed_3005.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d70e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xwind = Xwind.dropna(how='any', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6633489b",
   "metadata": {},
   "source": [
    "## Import Precip Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "616f7c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xprecip = read_pickle(r'C:\\Users\\filipovicha\\Documents\\AI_Project\\moov-ai-automatic-qc\\data\\processed_before_nf\\precipitation_amount_285.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f98b4f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xprecip = Xprecip.dropna(how='any', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15acc5a1",
   "metadata": {},
   "source": [
    "## Detect Snow Depth Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f931f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_snow_elems = list(np.intersect1d(Xsnow.columns, newDf.columns))\n",
    "test_real_time = newDf[common_snow_elems] \n",
    "common_snow_elems.append('snow_depth_3022_target')\n",
    "train_real_time = Xsnow[common_snow_elems]\n",
    "train_y = train_real_time['snow_depth_3022_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e945e2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-a09e4313c62b>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_real_time['hour_of_day'] = train_real_time.date_time.dt.hour\n",
      "<ipython-input-19-a09e4313c62b>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_real_time['yearz'] = train_real_time.date_time.dt.year\n",
      "<ipython-input-19-a09e4313c62b>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_real_time['monthz'] = train_real_time.date_time.dt.month\n",
      "<ipython-input-19-a09e4313c62b>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_real_time['dayz'] = train_real_time.date_time.dt.day\n",
      "<ipython-input-19-a09e4313c62b>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_real_time['hour_of_day'] = train_real_time.date_time.dt.hour\n",
      "<ipython-input-19-a09e4313c62b>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_real_time['yearz'] = train_real_time.date_time.dt.year\n",
      "<ipython-input-19-a09e4313c62b>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_real_time['monthz'] = train_real_time.date_time.dt.month\n",
      "<ipython-input-19-a09e4313c62b>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_real_time['dayz'] = train_real_time.date_time.dt.day\n"
     ]
    }
   ],
   "source": [
    "# keeping the the date and time for anomaly detection\n",
    "train_real_time['hour_of_day'] = train_real_time.date_time.dt.hour\n",
    "train_real_time['yearz'] = train_real_time.date_time.dt.year\n",
    "train_real_time['monthz'] = train_real_time.date_time.dt.month\n",
    "train_real_time['dayz'] = train_real_time.date_time.dt.day\n",
    "\n",
    "test_real_time['hour_of_day'] = train_real_time.date_time.dt.hour\n",
    "test_real_time['yearz'] = train_real_time.date_time.dt.year\n",
    "test_real_time['monthz'] = train_real_time.date_time.dt.month\n",
    "test_real_time['dayz'] = train_real_time.date_time.dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42a3e339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filipovicha\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "del_cols = ['origin_filename', 'station_time_identifier', 'tc_identifier', 'date_time',\n",
    "            'station_name', 'version', 'correction', 'source_uri', 'station_identifier', '_merge']\n",
    "\n",
    "\n",
    "train_real_time.drop(del_cols , axis = 1, inplace=True, errors='ignore') \n",
    "test_real_time.drop(del_cols , axis = 1, inplace=True, errors='ignore') \n",
    "\n",
    "# removing target variable form training set\n",
    "train_real_time.drop('snow_depth_3022_target', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98d6aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Actual real-Time Train and Test\n",
    "rfc_estimator2 = rfc(n_estimators=1000, \n",
    "                    min_samples_leaf=2, \n",
    "                    n_jobs=7)\n",
    "sampler = ADASYN()\n",
    "clf = make_pipeline(sampler, rfc_estimator2)\n",
    "    \n",
    "rezult = clf.fit(train_real_time, train_y).predict(test_real_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59524a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-8812fbfd079a>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_real_time['classes'] = classification\n",
      "<ipython-input-22-8812fbfd079a>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_real_time['url'] = url_list\n"
     ]
    }
   ],
   "source": [
    "# merging the prediction/classification to the observations\n",
    "classification = list(rezult)\n",
    "test_real_time['classes'] = classification\n",
    "test_real_time['url'] = url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94b28ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "overturn = test_real_time.loc[test_real_time['classes'] == 1]\n",
    "snow_list = overturn['url']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63431f47",
   "metadata": {},
   "source": [
    "## Detect Wind Speed Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e12f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_cols = ['origin_filename', 'station_time_identifier', 'tc_identifier', 'date_time',\n",
    "            'station_name', 'version', 'correction', 'source_uri', 'station_identifier', '_merge']\n",
    "\n",
    "\n",
    "train_real_time.drop(del_cols , axis = 1, inplace=True, errors='ignore') \n",
    "test_real_time.drop(del_cols , axis = 1, inplace=True, errors='ignore') \n",
    "\n",
    "# removing target variable form training set\n",
    "train_real_time.drop('wind_speed_3005_target', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Actual real-Time Train and Test\n",
    "rfc_estimator2 = rfc(n_estimators=1000, \n",
    "                    min_samples_leaf=2, \n",
    "                    n_jobs=7)\n",
    "sampler = ADASYN()\n",
    "clf = make_pipeline(sampler, rfc_estimator2)\n",
    "    \n",
    "rezult = clf.fit(train_real_time, train_y).predict(test_real_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dad4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the prediction/classification to the observations\n",
    "classification = list(rezult)\n",
    "test_real_time['classes'] = classification\n",
    "test_real_time['url'] = url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309dc39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "overturn = test_real_time.loc[test_real_time['classes'] == 1]\n",
    "wind_list = overturn['url']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b242f2f7",
   "metadata": {},
   "source": [
    "## Detect Precip Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe428c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_precip_elems = list(np.intersect1d(Xprecip.columns, newDf.columns))\n",
    "test_real_time = newDf[common_precip_elems] \n",
    "common_precip_elems.append('precipitation_amount_285_target')\n",
    "train_real_time = Xprecip[common_precip_elems]\n",
    "train_y = train_real_time['precipitation_amount_285_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989df2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_cols = ['origin_filename', 'station_time_identifier', 'tc_identifier', 'date_time',\n",
    "            'station_name', 'version', 'correction', 'source_uri', 'station_identifier', '_merge']\n",
    "\n",
    "\n",
    "train_real_time.drop(del_cols , axis = 1, inplace=True, errors='ignore') \n",
    "test_real_time.drop(del_cols , axis = 1, inplace=True, errors='ignore') \n",
    "\n",
    "# removing target variable form training set\n",
    "train_real_time.drop('precipitation_amount_285_target', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6107ba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Actual real-Time Train and Test\n",
    "rfc_estimator2 = rfc(n_estimators=1000, \n",
    "                    min_samples_leaf=2, \n",
    "                    n_jobs=7)\n",
    "sampler = ADASYN()\n",
    "clf = make_pipeline(sampler, rfc_estimator2)\n",
    "    \n",
    "rezult = clf.fit(train_real_time, train_y).predict(test_real_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2275ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the prediction/classification to the observations\n",
    "classification = list(rezult)\n",
    "test_real_time['classes'] = classification\n",
    "test_real_time['url'] = url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b2642",
   "metadata": {},
   "outputs": [],
   "source": [
    "overturn = test_real_time.loc[test_real_time['classes'] == 1]\n",
    "precip_list = overturn['url']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dcff43",
   "metadata": {},
   "source": [
    "## Present data from list output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024d6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(source: str):\n",
    "    newstring = source.rsplit('/')\n",
    "    date = newstring[10] \n",
    "    stat = newstring[12]\n",
    "    return [date, stat]\n",
    " \n",
    "def make_dict(sources: list):\n",
    "    dicto = {}\n",
    "    for source in sources:\n",
    "        stat = extract_info(source)[1]\n",
    "        date = extract_info(source)[0]\n",
    "        \n",
    "        #check if stat is in sources:\n",
    "        if stat not in dicto.keys():\n",
    "            dicto[stat] = {'count': 1, 'earliest' : date, 'latest' : date}\n",
    "            \n",
    "        else:\n",
    "            if date < dicto[stat]['earliest']:\n",
    "                dicto[stat]['count'] = dicto[stat]['count'] + 1\n",
    "                dicto[stat]['earliest'] = date\n",
    "                \n",
    "            elif date > dicto[stat]['latest']:\n",
    "                dicto[stat]['count'] = dicto[stat]['count'] + 1\n",
    "                dicto[stat]['latest'] = date\n",
    "                \n",
    "            else:\n",
    "                dicto[stat]['count'] = dicto[stat]['count'] + 1\n",
    "    \n",
    "    return dicto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e58d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_depth_anomalies = make_dict(snow_list)\n",
    "wind_speed_anomalies = make_dict(wind_list)\n",
    "precip_anomalies = make_dict(precip_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78410fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Snow Depth Anomalies')\n",
    "print(snow_depth_anomalies)\n",
    "\n",
    "print('Wind Speed Anomalies')\n",
    "print(wind_speed_anomalies)\n",
    "\n",
    "print('Precip Anomalies')\n",
    "print(precip_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e054ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('snow depth, std-pkg-id=\"1.11.174.2.5.3.0\"')\n",
    "\n",
    "for key in snow_depth_anomalies.keys():\n",
    "    early_year = snow_depth_anomalies[key]['earliest'][0] + snow_depth_anomalies[key]['earliest'][1] + snow_depth_anomalies[key]['earliest'][2] +snow_depth_anomalies[key]['earliest'][3]\n",
    "    early_month = snow_depth_anomalies[key]['earliest'][4] + snow_depth_anomalies[key]['earliest'][5]\n",
    "    early_day = snow_depth_anomalies[key]['earliest'][6] + snow_depth_anomalies[key]['earliest'][7]\n",
    "    early_hour = snow_depth_anomalies[key]['earliest'][8] + snow_depth_anomalies[key]['earliest'][9]\n",
    "    late_year = snow_depth_anomalies[key]['latest'][0] + snow_depth_anomalies[key]['latest'][1] + snow_depth_anomalies[key]['latest'][2] +snow_depth_anomalies[key]['latest'][3]\n",
    "    late_month = snow_depth_anomalies[key]['latest'][4] + snow_depth_anomalies[key]['latest'][5]\n",
    "    late_day = snow_depth_anomalies[key]['latest'][6] + snow_depth_anomalies[key]['latest'][7]\n",
    "    late_hour = snow_depth_anomalies[key]['latest'][8] + snow_depth_anomalies[key]['latest'][9]\n",
    "    url = 'https://dw.cmc.ec.gc.ca/services/pegasus/viewer/?stationID=' + key + '&from=' + str(early_year) + '-' + str(early_month) + '-' + str(early_day) + 'T' + str(early_hour) + ':00&to=' + str(late_year) + '-' + str(late_month) + '-' + str(late_day) + 'T' + str(late_hour) + ':00'\n",
    "    print(url + ' with count ' + str(snow_depth_anomalies[key]['count']))\n",
    "    \n",
    "print('precipitation amount, std-pkg-id=\"1.11.171.1.60.5.0\"')\n",
    "\n",
    "for key in precip_anomalies.keys():\n",
    "    early_year = precip_anomalies[key]['earliest'][0] + precip_anomalies[key]['earliest'][1] + precip_anomalies[key]['earliest'][2] +precip_anomalies[key]['earliest'][3]\n",
    "    early_month = precip_anomalies[key]['earliest'][4] + precip_anomalies[key]['earliest'][5]\n",
    "    early_day = precip_anomalies[key]['earliest'][6] + precip_anomalies[key]['earliest'][7]\n",
    "    early_hour = precip_anomalies[key]['earliest'][8] + precip_anomalies[key]['earliest'][9]\n",
    "    late_year = precip_anomalies[key]['latest'][0] + precip_anomalies[key]['latest'][1] + precip_anomalies[key]['latest'][2] +precip_anomalies[key]['latest'][3]\n",
    "    late_month = precip_anomalies[key]['latest'][4] + precip_anomalies[key]['latest'][5]\n",
    "    late_day = precip_anomalies[key]['latest'][6] + precip_anomalies[key]['latest'][7]\n",
    "    late_hour = precip_anomalies[key]['latest'][8] + precip_anomalies[key]['latest'][9]\n",
    "    url = 'https://dw.cmc.ec.gc.ca/services/pegasus/viewer/?stationID=' + key + '&from=' + str(early_year) + '-' + str(early_month) + '-' + str(early_day) + 'T' + str(early_hour) + ':00&to=' + str(late_year) + '-' + str(late_month) + '-' + str(late_day) + 'T' + str(late_hour) + ':00'\n",
    "    print(url + ' with count ' + str(precip_anomalies[key]['count']))\n",
    "\n",
    "print('wind speed, std-pkg-id=\"1.24.314.2.2.2.6\"')\n",
    "\n",
    "for key in wind_speed_anomalies.keys():\n",
    "    early_year = wind_speed_anomalies[key]['earliest'][0] + wind_speed_anomalies[key]['earliest'][1] + wind_speed_anomalies[key]['earliest'][2] +wind_speed_anomalies[key]['earliest'][3]\n",
    "    early_month = wind_speed_anomalies[key]['earliest'][4] + wind_speed_anomalies[key]['earliest'][5]\n",
    "    early_day = wind_speed_anomalies[key]['earliest'][6] + wind_speed_anomalies[key]['earliest'][7]\n",
    "    early_hour = wind_speed_anomalies[key]['earliest'][8] + wind_speed_anomalies[key]['earliest'][9]\n",
    "    late_year = wind_speed_anomalies[key]['latest'][0] + wind_speed_anomalies[key]['latest'][1] + wind_speed_anomalies[key]['latest'][2] +wind_speed_anomalies[key]['latest'][3]\n",
    "    late_month = wind_speed_anomalies[key]['latest'][4] + wind_speed_anomalies[key]['latest'][5]\n",
    "    late_day = wind_speed_anomalies[key]['latest'][6] + wind_speed_anomalies[key]['latest'][7]\n",
    "    late_hour = wind_speed_anomalies[key]['latest'][8] + wind_speed_anomalies[key]['latest'][9]\n",
    "    url = 'https://dw.cmc.ec.gc.ca/services/pegasus/viewer/?stationID=' + key + '&from=' + str(early_year) + '-' + str(early_month) + '-' + str(early_day) + 'T' + str(early_hour) + ':00&to=' + str(late_year) + '-' + str(late_month) + '-' + str(late_day) + 'T' + str(late_hour) + ':00'\n",
    "    print(url + ' with count ' + str(wind_speed_anomalies[key]['count']))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
